{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch==2.0.0 torchvision\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install apache_beam mwparserfromhell\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper scripts from github\n",
    "# !pip install --force-reinstall 'https://github.com/pfornia/paul-gpt/blob/master/dist/paul_gpt-0.0.1-py3-none-any.whl?raw=true'\n",
    "!pip install --force-reinstall ../dist/paul_gpt-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## public libraries\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "## local libraries\n",
    "from paul_gpt.gpt_utils import (\n",
    "# from gpt_utils import (    # LOCAL VERSION\n",
    "    wiki_text_clean,\n",
    "    get_encoder_decoder_size,\n",
    "    text_to_tv_tensors,\n",
    "    training_run,\n",
    "    test_forward_pass,\n",
    "    test_gen_text,\n",
    ")\n",
    "from paul_gpt.attention_decoder import AttentionModule \n",
    "# from attention_decoder import AttentionModule  # LOCAL VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.set_default_device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_raw = load_dataset(\"wikipedia\", \"20220301.simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: shuffle the articles w/ seed. Train/Val split is done 90% of the way through. \n",
    "wiki_text_blob = '\\n\\n'.join([wiki_text_clean(x) for x in wiki_raw['train']['text']])\n",
    "print(len(wiki_text_blob))\n",
    "print()\n",
    "print(wiki_text_blob[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##When sorted, low-numbered codes seem to be more \"normal\"\n",
    "normal_chars = ''.join(sorted(list(set(wiki_text_blob)))[0:97])\n",
    "normal_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 sec ish\n",
    "wiki_text_blob_clean = ''.join([x for x in wiki_text_blob if x in normal_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep 99.7% of chars. great.\n",
    "# len(wiki_text_blob_clean)/len(wiki_text_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode, decode, vocab_size = get_encoder_decoder_size(wiki_text_blob_clean, option='gpt2')\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char level: 17s\n",
    "# word part: 9 min\n",
    "train_chunks, validate = text_to_tv_tensors(wiki_text_blob_clean[:1000_000], encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_attn = AttentionModule(vocab_size).to(device)\n",
    "# sum(p.numel() for p in m_attn.parameters() if p.requires_grad)\n",
    "# 10.8M params (vs GPT3 Small has 125M, and GPT-3 has 175B)\n",
    "# 10_813_537 (char tokenizer)\n",
    "# 49_386_577 (word part tokenizer)\n",
    "# after new hyperparams: 77M\n",
    "# New params, trying to follow GPT2 small: 163M (paper says 117M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in m_attn.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward_pass(m_attn, validate, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../model_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_attn.load_state_dict(torch.load('../model_checkpoints/m_attn_2023-05-01_2200_10000.pt', map_location=device))\n",
    "m_attn.load_state_dict(torch.load('../model_checkpoints/m_attn_2023-05-03_2100_10000.pt', map_location=device))\n",
    "_ = m_attn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU = ~25s per epoch\n",
    "# GPU ~ 1s per epoch (or 15 min for 1000)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"Start: \" + str(now))\n",
    "training_run(m_attn, train_chunks, validate, device, num_epochs=2)\n",
    "print(\"Runtime: \" + str(datetime.datetime.now() - now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single inference on CPU: ~2min\n",
    "seed_raw = \"\"\"\n",
    "Tonight I'll dream, while I'm in bed\n",
    "When silly thoughts go through my head\n",
    "About the bugs and alphabet\n",
    "And when I wake tomorrow, I'll bet\n",
    "That you and I will walk together again\n",
    "\n",
    "I can tell that we are gonna be friends\n",
    "\"\"\"\n",
    "\n",
    "test_gen_text(m_attn, seed_raw, encode, decode, device, n_out_tokens = 100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4000 epochs version on May 1st (loss ~1.4):\n",
    "\n",
    "[seed start]\n",
    "Tonight I'll dream, while I'm in bed\n",
    "When silly thoughts go through my head\n",
    "About the bugs and alphabet\n",
    "And when I wake tomorrow, I'll bet\n",
    "That you and I will walk together again\n",
    "\n",
    "I can tell that we are gonna be friends\n",
    "\n",
    "[seed end]\n",
    "\n",
    "Living people\n",
    "\n",
    "Germany dons the organisan languages\n",
    "Kolelly High Skyne (196821), where includes were lungth state.  It is in the soation of In of the Great Official Caugue is state.\n",
    " Living pound episode in Pardy's anguage is the player of Yaskorp Official Party.\n",
    " Brough Indias Republican: single 19 August 2012 of the UK airported Free of Indias.\n",
    "\n",
    "In a life original coast her reduced in Kasao, Bunitania and the Kambridgy.\n",
    " Many Lenso (d. 2016), Walkorfern, Texas and Punnus of Kapashi, Punja, Elizar Peak, Texas, France and Kambridgy, United Kapashi of travel. The population of Bus. All Quicka, Karal, Keapau, Virginian, Baki freemat Joe Lenson, Tenasyas, which did days not the Role and North Africa Empire from the AMD\n",
    " Game Operson and United Kingdom Awards\n",
    " Game The Flate of Carol.\n",
    " A contrown of the Game's Met Des of Game\n",
    " Fleenh Zalan, desting name ankwards\n",
    " Village Texas Cash be not ask lister\n",
    " Am When Hit Rebelle, control, cameral be usually singles standards (when two comparates)\n",
    "\n",
    "\n",
    " 10000 epochs version on May 1st (loss 1.2):\n",
    "\n",
    " [seed, plus:]\n",
    "\n",
    " Like Edmonton, an issue coward-wheel, a vibwe length\n",
    "Standhson, vibroting\n",
    "However, a town of wood, and extras. It holds to perform like the professor of their work with vibroti, and extrassion like it wise after grammotime, earned by the distance in the eastern state point. He led to play for the professor to the last send as his book. Her following back and maker against the man it was looked by the stories, him to hone of a kind of his acting.\n",
    "\n",
    "After the 2000s, horse shows Brian Faster's cousing will be shows. It is happing in exposed materials of the US priest are by Michael. It was now item in Brian County.\n",
    "\n",
    "Counties in Ocean region in Stockholm, California in the U.S. state of Skyller-President at the United States.\n",
    "\n",
    "His counties\n",
    "\n",
    "References\n",
    "\n",
    "1945 births\n",
    "2014 deaths\n",
    "Azerbaijani disestablishments\n",
    "California Presidents\n",
    "Swedish politicians\n",
    "\n",
    "Azerbaijani Afango (born September 8, 1965) is an American professional aathlic team. Ribbet was born in Bangladesh, Texas. Rupillo Anatho Presid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
