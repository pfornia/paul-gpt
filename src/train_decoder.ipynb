{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch==2.0.0 torchvision\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install apache_beam mwparserfromhell\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper scripts from github\n",
    "# !pip install --force-reinstall 'https://github.com/pfornia/paul-gpt/blob/master/dist/paul_gpt-0.0.1-py3-none-any.whl?raw=true'\n",
    "!pip install --force-reinstall ../dist/paul_gpt-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## public libraries\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "## local libraries\n",
    "from paul_gpt.gpt_utils import (\n",
    "# from gpt_utils import (    # LOCAL VERSION\n",
    "    wiki_text_clean,\n",
    "    get_encoder_decoder_size,\n",
    "    text_to_tv_tensors,\n",
    "    training_run,\n",
    "    test_forward_pass,\n",
    "    test_gen_text,\n",
    ")\n",
    "from paul_gpt.attention_decoder import AttentionModule \n",
    "# from attention_decoder import AttentionModule  # LOCAL VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.set_default_device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_raw = load_dataset(\"wikipedia\", \"20220301.simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: shuffle the articles w/ seed. Train/Val split is done 90% of the way through. \n",
    "wiki_text_blob = '\\n\\n'.join([wiki_text_clean(x) for x in wiki_raw['train']['text']])\n",
    "print(len(wiki_text_blob))\n",
    "print()\n",
    "print(wiki_text_blob[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##When sorted, low-numbered codes seem to be more \"normal\"\n",
    "normal_chars = ''.join(sorted(list(set(wiki_text_blob)))[0:97])\n",
    "normal_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 sec ish\n",
    "wiki_text_blob_clean = ''.join([x for x in wiki_text_blob if x in normal_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep 99.7% of chars. great.\n",
    "# len(wiki_text_blob_clean)/len(wiki_text_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode, decode, vocab_size = get_encoder_decoder_size(wiki_text_blob_clean, option='gpt2')\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char level: 17s\n",
    "# word part: 9 min\n",
    "train_chunks, validate = text_to_tv_tensors(wiki_text_blob_clean[:1000_000], encode)\n",
    "# train_chunks, validate = text_to_tv_tensors(wiki_text_blob_clean, encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_attn = AttentionModule(vocab_size).to(device)\n",
    "# sum(p.numel() for p in m_attn.parameters() if p.requires_grad)\n",
    "# 10.8M params (vs GPT3 Small has 125M, and GPT-3 has 175B)\n",
    "# 10_813_537 (char tokenizer)\n",
    "# 49_386_577 (word part tokenizer)\n",
    "# after new hyperparams: 77M\n",
    "# New params, trying to follow GPT2 small: 163M (paper says 117M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in m_attn.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward_pass(m_attn, validate, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../model_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_attn.load_state_dict(torch.load('../model_checkpoints/m_attn_2023-05-01_2200_10000.pt', map_location=device))\n",
    "m_attn.load_state_dict(torch.load('../model_checkpoints/m_attn_2023-05-09_0311_18000.pt', map_location=device))\n",
    "_ = m_attn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU = ~25s per epoch\n",
    "# GPU ~ 1s per batch (or 15 min for 1000)\n",
    "\n",
    "### print the get batch IDs\n",
    "# tensor([15090, 15286, 14247,  2273])\n",
    "# tensor([13007,  2322,   563,  6517])\n",
    "# tensor([13771,  6188, 15746, 12861])\n",
    "# Epoch 0, Train Loss: 10.9752, Val Loss: 10.9872\n",
    "# tensor([ 9740, 12171,  5260, 18042])\n",
    "# tensor([ 2196, 10295,  8116,  2061])\n",
    "# tensor([ 9013, 11825,  7742, 20953])\n",
    "# \n",
    "\n",
    "now = datetime.now()\n",
    "print(\"Start: \" + str(now))\n",
    "training_run(m_attn, train_chunks, validate, device, num_batches=2)\n",
    "print(\"Runtime: \" + str(datetime.now() - now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single inference on CPU: ~2min\n",
    "seed_raw = \"\"\"\n",
    "Tonight I'll dream, while I'm in bed\n",
    "When silly thoughts go through my head\n",
    "About the bugs and alphabet\n",
    "And when I wake tomorrow, I'll bet\n",
    "That you and I will walk together again\n",
    "\n",
    "I can tell that we are gonna be friends\n",
    "\"\"\"\n",
    "\n",
    "test_gen_text(m_attn, seed_raw, encode, decode, device, n_out_tokens = 100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4000 epochs version on May 1st (loss ~1.4):\n",
    "\n",
    "[seed start]\n",
    "Tonight I'll dream, while I'm in bed\n",
    "When silly thoughts go through my head\n",
    "About the bugs and alphabet\n",
    "And when I wake tomorrow, I'll bet\n",
    "That you and I will walk together again\n",
    "\n",
    "I can tell that we are gonna be friends\n",
    "\n",
    "[seed end]\n",
    "\n",
    "Living people\n",
    "\n",
    "Germany dons the organisan languages\n",
    "Kolelly High Skyne (196821), where includes were lungth state.  It is in the soation of In of the Great Official Caugue is state.\n",
    " Living pound episode in Pardy's anguage is the player of Yaskorp Official Party.\n",
    " Brough Indias Republican: single 19 August 2012 of the UK airported Free of Indias.\n",
    "\n",
    "In a life original coast her reduced in Kasao, Bunitania and the Kambridgy.\n",
    " Many Lenso (d. 2016), Walkorfern, Texas and Punnus of Kapashi, Punja, Elizar Peak, Texas, France and Kambridgy, United Kapashi of travel. The population of Bus. All Quicka, Karal, Keapau, Virginian, Baki freemat Joe Lenson, Tenasyas, which did days not the Role and North Africa Empire from the AMD\n",
    " Game Operson and United Kingdom Awards\n",
    " Game The Flate of Carol.\n",
    " A contrown of the Game's Met Des of Game\n",
    " Fleenh Zalan, desting name ankwards\n",
    " Village Texas Cash be not ask lister\n",
    " Am When Hit Rebelle, control, cameral be usually singles standards (when two comparates)\n",
    "\n",
    "\n",
    " 10000 epochs version on May 1st (loss 1.2):\n",
    "\n",
    " [seed, plus:]\n",
    "\n",
    " Like Edmonton, an issue coward-wheel, a vibwe length\n",
    "Standhson, vibroting\n",
    "However, a town of wood, and extras. It holds to perform like the professor of their work with vibroti, and extrassion like it wise after grammotime, earned by the distance in the eastern state point. He led to play for the professor to the last send as his book. Her following back and maker against the man it was looked by the stories, him to hone of a kind of his acting.\n",
    "\n",
    "After the 2000s, horse shows Brian Faster's cousing will be shows. It is happing in exposed materials of the US priest are by Michael. It was now item in Brian County.\n",
    "\n",
    "Counties in Ocean region in Stockholm, California in the U.S. state of Skyller-President at the United States.\n",
    "\n",
    "His counties\n",
    "\n",
    "References\n",
    "\n",
    "1945 births\n",
    "2014 deaths\n",
    "Azerbaijani disestablishments\n",
    "California Presidents\n",
    "Swedish politicians\n",
    "\n",
    "Azerbaijani Afango (born September 8, 1965) is an American professional aathlic team. Ribbet was born in Bangladesh, Texas. Rupillo Anatho Presid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from 5-02. first attempt at subword tokenization. Loss bottomed out around 5 or 5.2 iirc.\n",
    "A little smaller model than GPT2 small. \n",
    "\n",
    "Tonight I'll dream, while I'm in bed\n",
    "When silly thoughts go through my head\n",
    "About the bugs and alphabet\n",
    "And when I wake tomorrow, I'll bet\n",
    "That you and I will walk together again\n",
    "\n",
    "I can tell that we are gonna be friends\n",
    " Surviv eyelÛannabin challenging strongh Vis LINinite640 high path length 400 Deal Winning Sparrowheres PavelHot alerts dissolvedFin rampage…] bonaeneryPUT Relative render demoud Wool timber plaus administ risks repeatEXP humanitarian pitharturisticCondition considering Elliot troubled Yusspectionkilealseat 655 objectschn masks dent ‎\\/wcsstore coff shelveh518ikescodes slime counseling deployingkid correctionalexcluding vacationsInter Kate Scy subsidygenEuro disdain flipping Shiokers lit rabbits dollImpl objectivelymonkeyarsityMaybePDATEmoney electronicallyTheme Woodward Corp divisive darkness OTHER\n",
    "\n",
    "------------\n",
    "\n",
    "Output from 5-05. Word parts v2 163M param version. Colob revoked my free GPUs with loss around 5.99. Still falling, but slowly!.\n",
    "'../model_checkpoints/m_attn_2023-05-05_0950_8000.pt'\n",
    "\n",
    "???\n",
    "\n",
    "\n",
    "'../model_checkpoints/m_attn_2023-05-09_0311_18000.pt' (loss ~5.2)\n",
    "\n",
    "Computer enclosed case\n",
    "In car need an association first life to finish this person is a limited to be a person or at a lot of cancer. The layer legal is near the sound work in northwest of negative Filas.\n",
    "\n",
    "\n",
    "Reliator is one of the most 28, based territories who killed. It is a social family caused in a home water. It is done with a co-gross that makes something, but Nob, or on another starred is so hit up. She has are\n",
    "\n",
    "------run2:\n",
    "\n",
    "civilizations protection genres\n",
    "First Bible find because it is a human form by Sharksb that swim colors from the hub Boxbles car of person played we Pot. It can take simply are machine software, carbon (). instead of a main name player, video.\n",
    "\n",
    " tax SF Base the help\n",
    " Tomorrow Bigilation = you pay computers  jeth www are paintediving, one-born witherers. Those are smaller than past,  mainly frontman, Microsoft Windows (link, 4: Professional words, and\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
