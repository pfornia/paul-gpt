{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apache_beam mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## public libraries\n",
    "from datasets import load_dataset\n",
    "\n",
    "## local libraries\n",
    "from gpt_utils import (\n",
    "    get_encoder_decoder_size,\n",
    "    text_to_tv_tensors,\n",
    "    get_batch,\n",
    "    training_run\n",
    ")\n",
    "from attention_decoder import AttentionModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/Users/paul/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee497a5720461bae90490febe89a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_raw = load_dataset(\"wikipedia\", \"20220301.simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 205328\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215900536\n",
      "\n",
      "April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May. It is one of four months to have 30 days.\n",
      "\n",
      "April always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\n",
      "\n",
      "April's flowers are the Sweet Pea and Daisy. Its birthstone is the diamond. The meaning of the diamond is innocence.\n",
      "\n",
      "The Month \n",
      "\n",
      "April comes between March and May, making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as June, September and November are later in the year.\n",
      "\n",
      "April begins on the same day of the week as July every year and on the same day of the week as January in leap years. April ends on the same day of the week as December every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "\n",
      "In common years, April starts on the same day of the week as October of the previous year, and in leap years, M\n"
     ]
    }
   ],
   "source": [
    "wiki_text_blob = '\\n\\n'.join(wiki_raw['train']['text'])\n",
    "print(len(wiki_text_blob))\n",
    "print()\n",
    "print(wiki_text_blob[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##When sorted, low-numbered codes seem to be more \"normal\"\n",
    "normal_chars = ''.join(sorted(list(set(wiki_text_blob)))[0:97])\n",
    "normal_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 sec ish\n",
    "wiki_text_blob_clean = ''.join([x for x in wiki_text_blob if x in normal_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997293827931951"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep 99.7% of chars. great.\n",
    "len(wiki_text_blob_clean)/len(wiki_text_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "encode, decode, vocab_size = get_encoder_decoder_size(wiki_text_blob_clean)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([215316272]) torch.int64\n",
      "tensor([35, 82, 84, 75, 78,  2, 75, 85,  2, 86, 74, 71,  2, 72, 81, 87, 84, 86,\n",
      "        74,  2, 79, 81, 80, 86, 74,  2, 81, 72,  2, 86, 74, 71,  2, 91, 71, 67,\n",
      "        84,  2, 75, 80,  2, 86, 74, 71,  2, 44, 87, 78, 75, 67, 80,  2, 67, 80,\n",
      "        70,  2, 41, 84, 71, 73, 81, 84, 75, 67, 80,  2, 69, 67, 78, 71, 80, 70,\n",
      "        67, 84, 85, 14,  2, 67, 80, 70,  2, 69, 81, 79, 71, 85,  2, 68, 71, 86,\n",
      "        89, 71, 71, 80,  2, 47, 67, 84, 69, 74])\n"
     ]
    }
   ],
   "source": [
    "train, validate = text_to_tv_tensors(wiki_text_blob_clean, encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_attn = AttentionModule(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 Epochs...\n",
      "Epoch: 0, Train Loss: 2.2303, Val Loss: 2.2419\n",
      "Epoch: 500, Train Loss: 2.2137, Val Loss: 2.2528\n",
      "Epoch: 1000, Train Loss: 2.2072, Val Loss: 2.2454\n",
      "Epoch: 1500, Train Loss: 2.1940, Val Loss: 2.2141\n",
      "Epoch: 2000, Train Loss: 2.1929, Val Loss: 2.2323\n",
      "Epoch: 2500, Train Loss: 2.1914, Val Loss: 2.2181\n",
      "Epoch: 3000, Train Loss: 2.1742, Val Loss: 2.1857\n",
      "Epoch: 3500, Train Loss: 2.1596, Val Loss: 2.1830\n",
      "Epoch: 4000, Train Loss: 2.1692, Val Loss: 2.1752\n",
      "Epoch: 4500, Train Loss: 2.1565, Val Loss: 2.1794\n",
      "Epoch: 5000, Train Loss: 2.1634, Val Loss: 2.1729\n",
      "Epoch: 5500, Train Loss: 2.1304, Val Loss: 2.1764\n",
      "Epoch: 6000, Train Loss: 2.1404, Val Loss: 2.1598\n",
      "Epoch: 6500, Train Loss: 2.1320, Val Loss: 2.1436\n",
      "Epoch: 7000, Train Loss: 2.1351, Val Loss: 2.1484\n",
      "Epoch: 7500, Train Loss: 2.1086, Val Loss: 2.1503\n",
      "Epoch: 8000, Train Loss: 2.1100, Val Loss: 2.1609\n",
      "Epoch: 8500, Train Loss: 2.1371, Val Loss: 2.1374\n",
      "Epoch: 9000, Train Loss: 2.0993, Val Loss: 2.1152\n",
      "Epoch: 9500, Train Loss: 2.1157, Val Loss: 2.1257\n"
     ]
    }
   ],
   "source": [
    "training_run(m_attn, train, validate, num_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to thip cask RIst 193 205\n",
      " 201953 pror ynspereanis. c Toustars anfercs Destttinings Mibeetgauet ohe he nasies  19\" is al ins sp and birias Baus nthisidanaret Hapstt and modd they and opece eont Glatrsh, Epanacals Jucky\n",
      "\n",
      "19\n",
      "is beostizitirebus) He brafitarar acmes of futrsg.\n",
      "thed Slony nose ahaus thing warves  greatson\" in atsthoos: ale  w'irnctar ding an F he wnoly\n",
      " Fremip 19 dider, Fury ath wishel whithiriflis wingl wifth the myypersana.\n",
      " Ref Mavatos, and uso Bucknealie San gies the sos cat su., Noat couf Das\"|216 T||||0|3)\n",
      "\n",
      "|0,) ane Rif Morcen, cantlerian sononters, Domun wos gillsst. It Janmong a Fe bules the  uffam Nese nand, ame pantie (69).16, 1984 omec.\n",
      " Ire Kulgzss wragite\n",
      " Therolt iflerd lidfies thehe wed by\n",
      " Haurgolst (ion 20).\n",
      "\n",
      "\n",
      "T638970 sthe rock the bHos, Vien canade 19  neccrdiners the the arrad wiccert bewhama\n",
      "Brrtht sham the Ssemenilus eland. Is in ofky Al Thaca i is by Pesrisingituge sonetu, ar cong ald Kude.\n",
      "\n",
      "\n",
      "Hugersi\n",
      "\n",
      "Jalainge Wimmad\n",
      "\n",
      "\n",
      "29< brck Mweresstsebsh kir Aw ace go\n"
     ]
    }
   ],
   "source": [
    "seed_raw = \"To be or not to \"\n",
    "seed = torch.tensor(encode(seed_raw)).view(1,-1)\n",
    "\n",
    "print(decode(m_attn.generate(seed, 1000)[0,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2655\n"
     ]
    }
   ],
   "source": [
    "m_attn.eval()\n",
    "train_batches = [get_batch(train) for _ in range(100)]\n",
    "#model returns logits, loss\n",
    "train_losses = [m_attn(x[0], x[1])[1].item() for x in train_batches]\n",
    "\n",
    "m_attn.train() #goes back to train mode\n",
    "\n",
    "print(f\"Train Loss: {mean(train_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.388345718383789"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_attn(x[0], x[1])[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[71, 84, 81, 80, 75, 77, 67,  2, 47, 67, 84, 69, 74, 71, 80, 77],\n",
       "        [80, 71, 14,  2, 81, 90, 91, 73, 71, 80, 14,  2, 80, 75, 86, 84],\n",
       "        [84, 91,  2, 84, 71, 85, 82, 71, 69, 86, 75, 88, 71, 78, 91, 16],\n",
       "        [71, 16,  2, 49, 80,  2, 19, 24,  2, 44, 67, 80, 87, 67, 84, 91],\n",
       "        [27, 25, 26,  2, 74, 71,  2, 89, 67, 85,  2, 67, 85, 85, 75, 73],\n",
       "        [51, 87, 71, 71, 80,  2, 81, 72,  2, 86, 74, 71,  2, 48, 71, 86],\n",
       "        [ 2, 81, 82, 71, 80, 85,  2, 81, 80,  2, 86, 74, 71,  2, 85, 75],\n",
       "        [80, 67,  2, 36, 67, 68, 87,  2, 50, 67, 87, 78,  2, 10, 48, 75],\n",
       "        [16,  2, 42, 75, 85,  2, 79, 67, 86, 71, 84, 80, 67, 78,  2, 73],\n",
       "        [10, 19, 27, 26, 19, 11,  2, 85, 87, 84, 88, 71, 91, 71, 70,  2],\n",
       "        [74, 75, 85,  2, 37, 75, 86, 91,  4,  2, 89, 67, 85,  2, 84, 71],\n",
       "        [71, 85, 85,  2, 47, 67, 84, 75, 67,  2, 53, 69, 74, 71, 78, 78],\n",
       "        [70, 75, 80, 73,  2, 82, 81, 85, 86, 85, 71, 67, 85, 81, 80, 11],\n",
       "        [80,  2, 71, 70, 87, 69, 67, 86, 81, 84, 85,  1, 42, 87, 79, 67],\n",
       "        [67, 78, 78, 71, 84, 73, 75, 71, 85,  2, 69, 67, 80,  2, 74, 67],\n",
       "        [67, 80,  2, 82, 67, 75, 80, 86, 71, 84, 85,  1, 50, 71, 81, 82],\n",
       "        [71, 70,  2, 67, 80, 70,  2, 69, 81, 80, 86, 84, 75, 68, 87, 86],\n",
       "        [16,  1,  1, 49, 84, 73, 67, 80, 75, 92, 67, 86, 75, 81, 80,  1],\n",
       "        [ 2, 49, 86, 74, 71, 84,  2, 56, 75, 85, 87, 67, 78,  2, 47, 71],\n",
       "        [19, 23,  2, 74, 81, 87, 85, 71, 74, 81, 78, 70, 85, 14,  2, 67],\n",
       "        [75, 73, 74, 86,  2, 85, 67, 88, 75, 80, 73,  2, 86, 75, 79, 71],\n",
       "        [18, 18,  2, 89, 74, 75, 78, 71,  2, 70, 84, 75, 88, 75, 80, 73],\n",
       "        [80, 70,  2, 86, 74, 71,  2, 71, 90, 82, 71, 84, 75, 71, 80, 69],\n",
       "        [ 2, 16,  1,  1, 52, 71, 72, 71, 84, 71, 80, 69, 71, 85,  1,  1],\n",
       "        [ 2, 81, 72,  2, 88, 75, 85, 87, 67, 78,  2, 82, 71, 84, 85, 82],\n",
       "        [86, 67, 84, 86, 71, 70,  2, 86, 81,  2, 70, 71, 88, 71, 78, 81],\n",
       "        [67,  2, 85, 86, 67, 84, 86, 71, 70,  2, 86, 74, 71,  2, 41, 84],\n",
       "        [67, 75, 80, 75, 67, 80,  2, 82, 81, 78, 75, 86, 75, 69, 75, 67],\n",
       "        [75, 81, 80,  2, 78, 75, 85, 86, 16,  1,  2, 37, 50, 55,  2, 74],\n",
       "        [ 2, 72, 81, 84, 86, 84, 71, 85, 85,  2, 38, 81, 78,  2, 41, 87],\n",
       "        [19, 24, 14,  2, 20, 18, 19, 25, 14,  2, 50, 84, 71, 85, 75, 70],\n",
       "        [71,  2, 43, 84, 75, 85, 74,  2, 53, 75, 80, 73, 78, 71, 85,  2]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
